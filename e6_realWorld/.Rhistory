# this will remove responses from the dataframe that failed comprehension checks (i.e., "2")
d <- subset(d, ( d$comp_other == 2 & d$comp_options == d$order_num & d$comp_you == 1))
dim(d) # number of participants should decrease after comprehension exclusions
## get number of participants AFTER exclusions:
n_final <- dim(d)[1] # extracting number of rows only, not columns
n_final
percent_excluded <- (n_original - n_final)/n_original
percent_excluded
## age
mean(as.numeric(d$age), trim = 0, na.rm = TRUE) ## mean age
## gender
table(d$gender)[2]/sum(table(d$gender)) ## percentage of females
## ================================================================================================================
##                                                    SUBSETTING
## ================================================================================================================
d_merged <- array(dim=c(dim(d)[1]*2, 5))
colnames(d_merged) <- c('order', 'cond', 'selfish', 'deniability', 'friend')
d_merged <- as.data.frame(d_merged, stringsAsFactors=FALSE)
d_merged$order <- c(d$order, d$order)
d_merged$cond <- c(rep(c(1),each=dim(d)[1]), rep(c(2),each=dim(d)[1])) #1= 2 options, 2 = 3 options
d_merged$selfish <- c(d$cond1_selfish_1, d$cond2_selfish_1)
d_merged$deniability <- c(d$cond1_deniability_1, d$cond2_deniability_1)
d_merged$friend <- c(d$cond1_friend_1, d$cond2_friend_1)
## ================================================================================================================
##                              DATA ANALYSIS - SUMMARY, T-TESTS, AND LINEAR REGRESSION
## ================================================================================================================
#selfish
## (1) selfish
d_merged %>% group_by(cond) %>% get_summary_stats(selfish, type = "mean_sd")
selfish_mod <- lmer(selfish ~ cond + (1 | order), data=d_merged)
summary(selfish_mod)
## ================================================================================================================
##                                 Harvard Business School, Ethical Intelligence Lab
## ================================================================================================================
##                                DATA ANALYSIS | AV SCENARIOS | WITHIN SUBJECTS
## ================================================================================================================
## clear workspace
rm(list = ls())
options(download.file.method="libcurl")
## install packages
library(ggpubr)
library(rstatix)
if (!require(pacman)) {install.packages("pacman")}
pacman::p_load('ggplot2',         # plotting
'ggsignif',        # plotting significance bars
'lme4',            # functions for fitting linear regression models
'ggforce',         # make ggplot even fancier
'ggpubr',          # arrange plots in a grid, if needed
'ltm',             # probably not using..
'tidyr',           # tools for cleaning messy data
'stringr',         # perform string substitutions easily
'assertthat',      # allows me to check whether a variable is a string, with is.string
'lsmeans',         # contrast analysis for regression models
'stats',           # use function to adjust for multiple comparisons
'filesstrings',    # create and move files
'simr',            # power analysis for mixed models
'compute.es',      # effect size package
'effsize',         # another effect size package
'pwr',             # package for power calculation
'nlme',            # get p values for mixed effect model
'DescTools',       # get Cramer's V
'Hmisc',
'effsize'          # effect size
)
library("lmerTest")
## ================================================================================================================
##                                                  PRE-PROCESSING
## ================================================================================================================
## read in data:
# if importing from Qualtrics: (i) export data as numeric values, and (ii) delete rows 2 and 3 of the .csv file.
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #set working directory to current directory
d <- read.csv('e5_data.csv')
## ================================================================================================================
##                                 Harvard Business School, Ethical Intelligence Lab
## ================================================================================================================
##                                DATA ANALYSIS | AV SCENARIOS | WITHIN SUBJECTS
## ================================================================================================================
## clear workspace
rm(list = ls())
options(download.file.method="libcurl")
## install packages
library(ggpubr)
library(rstatix)
if (!require(pacman)) {install.packages("pacman")}
pacman::p_load('ggplot2',         # plotting
'ggsignif',        # plotting significance bars
'lme4',            # functions for fitting linear regression models
'ggforce',         # make ggplot even fancier
'ggpubr',          # arrange plots in a grid, if needed
'ltm',             # probably not using..
'tidyr',           # tools for cleaning messy data
'stringr',         # perform string substitutions easily
'assertthat',      # allows me to check whether a variable is a string, with is.string
'lsmeans',         # contrast analysis for regression models
'stats',           # use function to adjust for multiple comparisons
'filesstrings',    # create and move files
'simr',            # power analysis for mixed models
'compute.es',      # effect size package
'effsize',         # another effect size package
'pwr',             # package for power calculation
'nlme',            # get p values for mixed effect model
'DescTools',       # get Cramer's V
'Hmisc',
'effsize'          # effect size
)
library("lmerTest")
## ================================================================================================================
##                                                  PRE-PROCESSING
## ================================================================================================================
## read in data:
# if importing from Qualtrics: (i) export data as numeric values, and (ii) delete rows 2 and 3 of the .csv file.
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #set working directory to current directory
d <- read.csv('e5_data.csv')
## perform attention exclusions:
# this will remove responses from the dataframe that failed attention checks (i.e., "1" or "2")
d <- subset(d, (d$att1 == 2 & d$att2 == 2))
dim(d)
## ================================================================================================================
##                                 Harvard Business School, Ethical Intelligence Lab
## ================================================================================================================
##                                DATA ANALYSIS | AV SCENARIOS | WITHIN SUBJECTS
## ================================================================================================================
## clear workspace
rm(list = ls())
options(download.file.method="libcurl")
## install packages
library(ggpubr)
library(rstatix)
if (!require(pacman)) {install.packages("pacman")}
pacman::p_load('ggplot2',         # plotting
'ggsignif',        # plotting significance bars
'lme4',            # functions for fitting linear regression models
'ggforce',         # make ggplot even fancier
'ggpubr',          # arrange plots in a grid, if needed
'ltm',             # probably not using..
'tidyr',           # tools for cleaning messy data
'stringr',         # perform string substitutions easily
'assertthat',      # allows me to check whether a variable is a string, with is.string
'lsmeans',         # contrast analysis for regression models
'stats',           # use function to adjust for multiple comparisons
'filesstrings',    # create and move files
'simr',            # power analysis for mixed models
'compute.es',      # effect size package
'effsize',         # another effect size package
'pwr',             # package for power calculation
'nlme',            # get p values for mixed effect model
'DescTools',       # get Cramer's V
'Hmisc',
'effsize'          # effect size
)
library("lmerTest")
## ================================================================================================================
##                                                  PRE-PROCESSING
## ================================================================================================================
## read in data:
# if importing from Qualtrics: (i) export data as numeric values, and (ii) delete rows 2 and 3 of the .csv file.
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #set working directory to current directory
d <- read.csv('e5_data.csv')
## perform attention exclusions:
# this will remove responses from the dataframe that failed attention checks (i.e., "1" or "2")
d <- subset(d, (d$att1 == 2 & d$att2 == 2))
dim(d)
## ================================================================================================================
##                                              PERFORM EXCLUSIONS
## ================================================================================================================
## get number of participants BEFORE exclusions:
n_original <- dim(d)[1] # extracting number of rows only, not columns
n_original
d$order <- ifelse(d$FL_27_DO=='FL_37', 'two_three', 'three_two')
d$order_num <- ifelse(d$order=='two_three', 2, 1)
## perform comprehension exclusions:
# this will remove responses from the dataframe that failed comprehension checks (i.e., "2")
d <- subset(d, ( d$comp_other == 2 & d$comp_options == d$order_num & d$comp_you == 1))
dim(d) # number of participants should decrease after comprehension exclusions
## get number of participants AFTER exclusions:
n_final <- dim(d)[1] # extracting number of rows only, not columns
n_final
percent_excluded <- (n_original - n_final)/n_original
percent_excluded
## age
mean(as.numeric(d$age), trim = 0, na.rm = TRUE) ## mean age
## gender
table(d$gender)[2]/sum(table(d$gender)) ## percentage of females
## ================================================================================================================
##                                                    SUBSETTING
## ================================================================================================================
d_merged <- array(dim=c(dim(d)[1]*2, 5))
colnames(d_merged) <- c('order', 'cond', 'selfish', 'deniability', 'friend')
d_merged <- as.data.frame(d_merged, stringsAsFactors=FALSE)
d_merged$order <- c(d$order, d$order)
d_merged$cond <- c(rep(c(1),each=dim(d)[1]), rep(c(2),each=dim(d)[1])) #1= 2 options, 2 = 3 options
d_merged$selfish <- c(d$cond1_selfish_1, d$cond2_selfish_1)
d_merged$deniability <- c(d$cond1_deniability_1, d$cond2_deniability_1)
d_merged$friend <- c(d$cond1_friend_1, d$cond2_friend_1)
## ================================================================================================================
##                              DATA ANALYSIS - SUMMARY, T-TESTS, AND LINEAR REGRESSION
## ================================================================================================================
#selfish
## (1) selfish
d_merged %>% group_by(cond) %>% get_summary_stats(selfish, type = "mean_sd")
selfish_mod <- lmer(selfish ~ cond + (1 | order), data=d_merged)
summary(selfish_mod)
selfish_mod <- lmer(selfish ~ cond + order, data=d_merged)
selfish_mod <- lm(selfish ~ cond + order, data=d_merged)
summary(selfish_mod)
## (2) deniability
d_merged %>% group_by(cond) %>% get_summary_stats(deniability, type = "mean_sd")
deniability_mod <- lmer(deniability ~ cond + (1 | order), data=d_merged)
summary(deniability_mod)
## (2) friend
d_merged %>% group_by(cond) %>% get_summary_stats(friend, type = "mean_sd")
friend_mod <- lmer(friend ~ cond + (1 | order), data=d_merged)
summary(friend_mod)
#pk
mean(d$pk_egal_1)
# Julian De Freitas, 2021
# Analysis script for De Freitas
# E4, Should Automated Vehicles Favor Passengers Over Pedestrians?
## clear workspace
rm(list = ls())
options(download.file.method="libcurl")
if (!require(pacman)) {install.packages("pacman")}
pacman::p_load('ggplot2', #plotting
'ggsignif', #plotting significance bars
'lme4', #functions for fitting linear regression modesl
'ggforce', #make ggplot even fancier
'ggpubr', #arrange plots in a grid, if neededd
'ltm', #probably not using..
'simr', # power analysis for mixed models
'compute.es', # effect size package
'effsize', # another effect size package
'pwr', #package for power calculation
)
library("lmerTest")
##================================================================================================================
##IMPORT & PRE-PROCESS d##
##================================================================================================================
#get subjects from e1
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #set working directory to current directory
#read data
d_raw <- read.csv('e7_data.csv')
d_raw <- subset(d_raw, d_raw$att1==2 & d_raw$att2==2)
#define new d frame that we'll extract preprocessed d into
d_subset <- array(dim=c(dim(d_raw)[1], 2))
colnames(d_subset) <- c('order_cond', 'choice')
d_subset <- as.data.frame(d_subset, stringsAsFactors=FALSE)
#extract the good d from the middle part of the raw d
for(i in 1:dim(d_raw)[1]) {
curr <- d_raw[i,c(14,16,18,20,22,24)][!is.na(d_raw[i,c(14,16,18,20,22,24)])] #for a given row, get only the non NA values
d_subset[i,2] <- as.numeric(curr[curr!= ""]) #and only the non-empty values
cond_names <- d_raw[i,37][!is.na(d_raw[i,37])]
d_subset[i,1] <- strsplit(cond_names[[1]], "page")[[1]][2]
}
#merge good d with first and last halves of the original d
d_raw <- cbind(d_subset, d_raw[,26:36])
# number of subjects before exclusions
n_original <- dim(d_raw)[1]
n_original
##================================================================================================================
##EXCLUSIONS##
##================================================================================================================
#perform exclusions based on attention and comprehension checks
d <- subset(d_raw, (d_raw$comp1 == 1) &
(d_raw$comp2 == 2))
#number of subjects after exclusions
n_after_exclusions <- dim(d)[1]
n_after_exclusions
n_original-n_after_exclusions
percent_excluded <- (n_original - n_after_exclusions)/n_original #34%
percent_excluded
##================================================================================================================
##ANALYSIS##
##================================================================================================================
# choices
results_table <- table(d$choice)
chisq.test(results_table)
results_table[1]/sum(results_table) #prosocial
results_table[2]/sum(results_table) #overt selfish
results_table[3]/sum(results_table) #unselfish alibi
results_table_subset <- table(d$choice)[c(2,3)]
chisq.test(results_table_subset)
#pk
mean(d$pk_egal_1)
sd(d$pk_egal_1)
mean(d$pk_selfish_1)
sd(d$pk_selfish_1)
mean(d$pk_alibi_1)
sd(d$pk_alibi_1)
var.test(d$pk_selfish_1, d$pk_alibi_1)
t.test(d$pk_selfish_1, d$pk_alibi_1, paired=TRUE, var.equal=TRUE)
#sk
mean(d$sk_egal_1)
sd(d$sk_egal_1)
mean(d$sk_selfish_1)
sd(d$sk_selfish_1)
mean(d$sk_alibi_1)
sd(d$sk_alibi_1)
var.test(d$pk_selfish_1, d$pk_alibi_1)
t.test(d$sk_selfish_1, d$sk_alibi_1, paired=TRUE, var.equal=TRUE)
#ck
mean(d$ck_egal_1)
sd(d$ck_egal_1)
mean(d$ck_selfish_1)
sd(d$ck_selfish_1)
mean(d$ck_alibi_1)
sd(d$ck_alibi_1)
var.test(d$ck_selfish_1, d$ck_alibi_1)
t.test(d$ck_selfish_1, d$ck_alibi_1, paired=TRUE, var.equal=TRUE)
# Julian De Freitas, 2021
# Analysis script for De Freitas
# E4, Should Automated Vehicles Favor Passengers Over Pedestrians?
## clear workspace
rm(list = ls())
options(download.file.method="libcurl")
if (!require(pacman)) {install.packages("pacman")}
pacman::p_load('ggplot2', #plotting
'ggsignif', #plotting significance bars
'lme4', #functions for fitting linear regression modesl
'ggforce', #make ggplot even fancier
'ggpubr', #arrange plots in a grid, if neededd
'ltm', #probably not using..
'simr', # power analysis for mixed models
'compute.es', # effect size package
'effsize', # another effect size package
'pwr', #package for power calculation
)
library("lmerTest")
##================================================================================================================
##IMPORT & PRE-PROCESS d##
##================================================================================================================
#get subjects from e1
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #set working directory to current directory
#read data
d_raw <- read.csv('e7_data.csv')
d_raw <- subset(d_raw, d_raw$att1==2 & d_raw$att2==2)
#define new d frame that we'll extract preprocessed d into
d_subset <- array(dim=c(dim(d_raw)[1], 2))
colnames(d_subset) <- c('order_cond', 'choice')
d_subset <- as.data.frame(d_subset, stringsAsFactors=FALSE)
#extract the good d from the middle part of the raw d
for(i in 1:dim(d_raw)[1]) {
curr <- d_raw[i,c(14,16,18,20,22,24)][!is.na(d_raw[i,c(14,16,18,20,22,24)])] #for a given row, get only the non NA values
d_subset[i,2] <- as.numeric(curr[curr!= ""]) #and only the non-empty values
cond_names <- d_raw[i,37][!is.na(d_raw[i,37])]
d_subset[i,1] <- strsplit(cond_names[[1]], "page")[[1]][2]
}
#merge good d with first and last halves of the original d
d_raw <- cbind(d_subset, d_raw[,26:36])
# number of subjects before exclusions
n_original <- dim(d_raw)[1]
n_original
##================================================================================================================
##EXCLUSIONS##
##================================================================================================================
#perform exclusions based on attention and comprehension checks
d <- subset(d_raw, (d_raw$comp1 == 1) |
(d_raw$comp2 == 2))
#number of subjects after exclusions
n_after_exclusions <- dim(d)[1]
n_after_exclusions
n_original-n_after_exclusions
percent_excluded <- (n_original - n_after_exclusions)/n_original #34%
percent_excluded
##================================================================================================================
##ANALYSIS##
##================================================================================================================
# choices
results_table <- table(d$choice)
chisq.test(results_table)
results_table[1]/sum(results_table) #prosocial
results_table[2]/sum(results_table) #overt selfish
results_table[3]/sum(results_table) #unselfish alibi
results_table_subset <- table(d$choice)[c(2,3)]
chisq.test(results_table_subset)
n_original
#perform exclusions based on attention and comprehension checks
d <- subset(d_raw, (d_raw$comp1 == 1) |
(d_raw$comp2 == 2))
#number of subjects after exclusions
n_after_exclusions <- dim(d)[1]
n_after_exclusions
# Julian De Freitas, 2021
# Analysis script for De Freitas
# E4, Should Automated Vehicles Favor Passengers Over Pedestrians?
## clear workspace
rm(list = ls())
options(download.file.method="libcurl")
if (!require(pacman)) {install.packages("pacman")}
pacman::p_load('ggplot2', #plotting
'ggsignif', #plotting significance bars
'lme4', #functions for fitting linear regression modesl
'ggforce', #make ggplot even fancier
'ggpubr', #arrange plots in a grid, if neededd
'ltm', #probably not using..
'simr', # power analysis for mixed models
'compute.es', # effect size package
'effsize', # another effect size package
'pwr', #package for power calculation
)
library("lmerTest")
##================================================================================================================
##IMPORT & PRE-PROCESS d##
##================================================================================================================
#get subjects from e1
setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) #set working directory to current directory
#read data
d_raw <- read.csv('e7_data.csv')
d_raw <- subset(d_raw, d_raw$att1==2 & d_raw$att2==2)
#define new d frame that we'll extract preprocessed d into
d_subset <- array(dim=c(dim(d_raw)[1], 2))
colnames(d_subset) <- c('order_cond', 'choice')
d_subset <- as.data.frame(d_subset, stringsAsFactors=FALSE)
#extract the good d from the middle part of the raw d
for(i in 1:dim(d_raw)[1]) {
curr <- d_raw[i,c(14,16,18,20,22,24)][!is.na(d_raw[i,c(14,16,18,20,22,24)])] #for a given row, get only the non NA values
d_subset[i,2] <- as.numeric(curr[curr!= ""]) #and only the non-empty values
cond_names <- d_raw[i,37][!is.na(d_raw[i,37])]
d_subset[i,1] <- strsplit(cond_names[[1]], "page")[[1]][2]
}
#merge good d with first and last halves of the original d
d_raw <- cbind(d_subset, d_raw[,26:36])
# number of subjects before exclusions
n_original <- dim(d_raw)[1]
n_original
##================================================================================================================
##EXCLUSIONS##
##================================================================================================================
#perform exclusions based on attention and comprehension checks
d <- subset(d_raw, (d_raw$comp1 == 1) |
(d_raw$comp2 == 2))
#number of subjects after exclusions
n_after_exclusions <- dim(d)[1]
n_after_exclusions
n_original-n_after_exclusions
percent_excluded <- (n_original - n_after_exclusions)/n_original #34%
percent_excluded
##================================================================================================================
##ANALYSIS##
##================================================================================================================
# choices
results_table <- table(d$choice)
chisq.test(results_table)
# choices
results_table <- table(d$choice)
chisq.test(results_table)
dim(d)
results_table[1]/sum(results_table) #prosocial
results_table[2]/sum(results_table) #overt selfish
results_table[3]/sum(results_table) #unselfish alibi
results_table_subset <- table(d$choice)[c(2,3)]
chisq.test(results_table_subset)
#pk
mean(d$pk_egal_1)
sd(d$pk_egal_1)
var.test(d$pk_selfish_1, d$pk_alibi_1)
t.test(d$pk_selfish_1, d$pk_alibi_1, paired=TRUE, var.equal=TRUE)
#pk
mean(d$pk_egal_1)
sd(d$pk_egal_1)
mean(d$pk_selfish_1)
sd(d$pk_selfish_1)
mean(d$pk_alibi_1)
mean(d$pk_selfish_1)
sd(d$pk_selfish_1)
mean(d$pk_alibi_1)
sd(d$pk_alibi_1)
var.test(d$pk_selfish_1, d$pk_alibi_1)
t.test(d$sk_selfish_1, d$sk_alibi_1, paired=TRUE, var.equal=TRUE)
var.test(d$ck_selfish_1, d$ck_alibi_1)
t.test(d$ck_selfish_1, d$ck_alibi_1, paired=TRUE, var.equal=TRUE)
#pk
mean(d$pk_egal_1)
sd(d$pk_egal_1)
mean(d$pk_selfish_1)
var.test(d$pk_egal_1, d$pk_alibi_1)
t.test(d$pk_egal_1, d$pk_alibi_1, paired=TRUE, var.equal=TRUE)
var.test(d$sk_egal_1, d$sk_alibi_1)
t.test(d$sk_egal_1, d$sk_alibi_1, paired=TRUE, var.equal=TRUE)
var.test(d$ck_egal_1, d$ck_alibi_1)
t.test(d$ck_egal_1, d$ck_alibi_1, paired=TRUE, var.equal=TRUE)
#pk
mean(d$pk_egal_1)
sd(d$pk_egal_1)
mean(d$pk_selfish_1)
sd(d$pk_selfish_1)
mean(d$pk_alibi_1)
sd(d$pk_alibi_1)
var.test(d$pk_selfish_1, d$pk_alibi_1)
t.test(d$pk_selfish_1, d$pk_alibi_1, paired=TRUE, var.equal=TRUE)
#sk
mean(d$sk_egal_1)
sd(d$sk_egal_1)
mean(d$sk_selfish_1)
sd(d$sk_selfish_1)
mean(d$sk_alibi_1)
sd(d$sk_alibi_1)
var.test(d$pk_selfish_1, d$pk_alibi_1)
t.test(d$sk_selfish_1, d$sk_alibi_1, paired=TRUE, var.equal=TRUE)
#ck
mean(d$ck_egal_1)
sd(d$ck_egal_1)
mean(d$ck_selfish_1)
sd(d$ck_selfish_1)
mean(d$ck_alibi_1)
sd(d$ck_alibi_1)
var.test(d$ck_selfish_1, d$ck_alibi_1)
t.test(d$ck_selfish_1, d$ck_alibi_1, paired=TRUE, var.equal=TRUE)
d$pk_diff <- d$pk_selfish_1 - d$pk_alibi_1
d_comp <- subset(d, d$choice != 1)
d_comp$choice <- d_comp$choice - 2
mylogit <- glm(choice ~ pk_diff, d = d_comp, family = "binomial")
summary(mylogit)
